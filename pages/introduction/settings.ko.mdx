# LLM 설정

프롬프트를 사용할 때 API를 통하거나 직접 LLM과 상호 작용을 하게 됩니다. 몇 가지 파라미터를 설정하여 프롬프트에 대해 다른 결과를 얻을 수 있습니다.

**Temperature** - 간단히 말하자면 'temperature'가 낮을수록, 가장 확률이 높은 토큰이 항상 선택된다는 의미에서 결과는 더 결정론적이 됩니다. 
temperature를 높이면 랜덤성이 높아져서 더 다양하거나 창의적인 출력을 하게 할 수 있습니다. 즉 다른 대답을 도출할 가능성이 높아질수 있게됩니다. 응용예로서 보다 사실적이고 간결한 응답을 하도록 하기 위한 사실 기반의 QA와 같은 작업에서 더 낮은 temperature 값을 사용할 수 있습니다. 시를 짓거나 또는 기타 창의적인 작업의 경우 temperature 값을 높이는 것이 도움이 될 수 있습니다.


**Top_p** - 마찬가지로 핵(nucleus) 샘플링이라는 temperature 샘플링 기술인 'top_p'를 사용하면 모델이 응답을 생성할 때 얼마나 결정론적인지 제어할 수 있습니다. 정확하고 사실에 입각한 답변을 원하면 이 값을 낮게 하세요. 보다 다양한 응답을 원하면 더 높은 값으로 하세요.


일반적으로 양쪽다 바꾸기보다 한쪽만 바꾸는것을 권합니다.

기본 예제를 시작하기 전에, 사용 중인 LLM 버전에 따라 결과가 다를 수 있음을 주의하시기 바랍니다.